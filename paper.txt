Batch Normalization导读 张俊林
Deep Reinforcement Learning 基础知识（DQN方面）(songrotek)
Human-level control through deep reinforcement learning
Paper Reading 3:Continuous control with Deep Reinforcement Learning(songrotek)
解密Google Deepmind AlphaGo围棋算法：真人工智能来自于哪里？(songrotek)
了解点OpenAI及深度学习研究前沿(songrotek)
深度学习计算模型中“门函数（Gating Function）”的作用(张俊林)
深度学习与自然语言处理之五：从RNN到LSTM(张俊林)
以Attention Model为例谈谈两种研究创新模式(张俊林)
自然语言处理中的Attention Model：是什么及为什么(张俊林)
李飞飞ICML2016

Bayesian Optimization
Javad Azimi Fall 2010 http://web.engr.oregonstate.edu/~azimi/ 

Entropy methods
www.stat.yale.edu/~pollard/Books/Asymptopia/old-Entropy.pdf

A Brief Introduction to Kolmogorov Complexity.pdf
Achieving the Gaussian Rate-Distortion Function by prediction.pdf
A Diversity-Promoting Objective Function for Neural Conversation Models.pdf
A Mathematical Theory of Learning.pdf
Analysis of commercial and free and open source solvers for linear optimation problems.pdf
A Neural Probabilistic Language Model.pdf
An Information Theoretic Perspective of the sparse coding.pdf
An Introduction to CRF for relational learning.pdf
ant colony optimization.pdf
A RATE-DISTORTION FRAMEWORK FOR SUPERVISED LEARNING.pdf
Artificial Convolution Neural Network for Medical Image.pdf
A Sparse-Response Deep Belief Network Based on Rate distortion theory.pdf
Basic Text Process.pdf
Bayesian Classification with gaussian process.pdf
bayesian-optimization.pptx
Bounded Rational Decision-Making in Feedforward Neural Networks.pdf
Challenges in Representation Learning A report on three machine learning contests.pdf
Compressive Sampling and Lossy Compression.pdf
Compressive Sensing.pdf
DEEP COMPRESSION(ICLR).pdf
Deep Face Recognition.pdf
Deep Learning Face Representation from Predicting 10,000 Classes.pdf
Deep Neural networks in machine translation_an overview.pdf
Discriminative Gaussian Process Latent models for calssification.pdf
Distances and affinities between measures.pdf
Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns.pdf
Entropy methods.pdf
Factorization of Latent Variables in Distributional Semantic Models.pdf
Fast-RCNN.pdf
Feed Forward Rate Distortion Function and Markov Sources.pdf
Fully Convolutional Networks for Semantic Segmentation.pdf
Gaussian Processes for Machine Learning.pdf
Generating Text with Recurrent Neural Networks.pdf
Generation and Comprehension of Unambiguous Object Descriptions.pdf
Hierarchical Clustering Based on Mutual Information.pdf
Introduction to Compressed sensing.pdf
Joint Word Representation Learning using a Corpus and a Semantic Lexicon.pdf
Kolmogorov Complexity.pdf
Machine Translation with LSTMs.pdf
Maximum Mutual Information Estimation with Unlabeled Data for phonetic classfication.PDF
Maximum mutual information regularized classification.pdf
Multi-Task Bayesian Optimization.pdf
Mutual Information and Diverse Decoding Improve Neural Machine translation.pdf
NEURAL MACHINE TRANSLATION by jointly learning to align and translate.pdf
neural word embeddings as implicit matrix factorization.pdf
On competitive prediction and its relation to rate-distortion theory.pdf
On the Properties of Neural Machine Translation_ Encoder–Decoder approach.pdf
ON THE RATE-DISTORTION PERFORMANCE OF COMPRESSED SENSING.pdf
Opportunistic Scheduling with Limited Channel State Information A Rate Distortion Approach.pdf
Perceptron Mistake Bounds.pdf
Random Matrices.pdf
random_report.pdf
senna-v3.0.tgz
Sequence to Sequence Learning with neural network.pdf
Shannon Information and Kolmogorov Complexity.pdf
Shannon Theoretic Limits on Noisy compressive sampling.pdf
Size and form in efficient transportation networks(nature).pdf
Structured Compressed Sensing_ from theory to application.pdf
Structured Recurrent Temporal Restricted Boltzmann Machines.pdf
sxzm.pdf
The Recurrent Temporal Restricted Boltzmann machine.pdf
thirty_years.txt
UFLDL(andrew ng).pdf
Unsupervised Feature Learning and Deep Learning_A Review and New Perspectives.pdf
VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION.pdf
Wide Residual Networks.pdf
幂律分布研究简史.pdf
激荡三十年-吴晓波.txt
神经网络的函数逼近理论.pdf
随机过程（Sheldon M.Ross 著）.pdf
作者：刘皮皮
链接：http://www.zhihu.com/question/31181823/answer/50954547
来源：知乎
著作权归作者所有，转载请联系作者获得授权。
Stacked convolutional auto-encoders for hierarchical feature extraction, Jonathan Masci, Ueli Meier, Dan Cire?san, and J¨urgen Schmidhuber, 2011
In visual object recognition, CNNs [1,3,4,14,26] often excel. Unlike patch based methods [19] they preserve the input’s neighborhood relations and spatial locality in their latent higher-level feature representations. While the common fully connected deep architectures do not scale well to realistic-sized high-dimensional images in terms of computational complexity, CNNs do, since the number of free parameters describing their shared weights does not depend on the input dimensionality.
Artificial convolution neural network for medical image pattern recognition
https://onedrive.live.com/redir?resid=DB4CF9BA40D4F042!4626&authkey=!AFfCWlXvZdm2X_Y&ithint=folder%2c
An Information Theoretic Perspective of the Sparse Coding
Ian Goodfellow, Yoshua Bengio and Aaron Courville <deep learning>

希尔伯特23个问题第二个：数学系统是相容的吗？
具体来说，完备性（证明或者证否），一致性（没有内部矛盾，反例如罗素悖论），可判定（用机械计算判定数学陈述的正确性）
哥德尔：包含算术的系统不能同时拥有一致性和完备性。
哥德尔：包含算术的系统，如果具有一致性，不能在系统内部证明该系统的一致性。

他先将所有的数学陈述和证明符号化，然后给每个符号串赋予一个数字，这个过程被称为哥德尔配数法。借助数学归纳法，我们可以建立针对所有自然数的陈述，而这样的陈述本身对应着一个数字，这个数字也符合陈述本身的要求。换言之，这个陈述陈述了它本身的性质。哥德尔正是通过这样魔法般的自指，完成了他的证明。（自指）

哥德尔：“一阶谓词演算”的逻辑系统是完备的，这被称为哥德尔完备性定理。
这个完备的逻辑系统是可判定的么？

图灵：不可判定（停机问题）。
通用图灵机（Universal Turing Machine）

停机问题证明涉及到自指的问题。 

函数，一个黑箱，f:a->b，这个a可以是实数，复数，事实上也可以是函数。。。（自指。。。）

但如果我们还记得哥德尔的教训的话，无所不能有时并不一定是什么好事，因为在数学和逻辑的领域中，对于有意义的逻辑系统，强大的表达能力必然伴随着坚不可摧的限制。如果一个系统无所不能，那么更大的可能是它本身就自相矛盾。

研究靠谱不靠谱，关键看研究人员靠谱不靠谱；我们需要了解手中的工具，才能更好地使用它，让它帮助我们回答我们想回答的问题；在学术界，永远不要高估自己的智商和知识，也永远不要低估别人的智商和知识；踏踏实实做研究，不要总想搞个大新闻
踏踏实实做人，老老实实做事！！！

科学松鼠会上的一系列博文：《计算的极限》。主要关于图灵机，可计算问题，lambda演算等等。这个系列博文比较复杂。

当前研究者投入了非常多的精力研究神经网络，包括深度信念网，深度玻尔兹曼机，自动编码机，去噪编码机，卷积神经网络等。在这些网络中，全连接层非常主要的构件，例如，深度信念网，自动编码机这样的网络所有层都是全连接层；卷积神经网络会增加几层全连接层以获得较好的分类准确度。全连接层的主要问题有两个：第一，表达能力太强，这个问题主要表现在训练容易过拟合。第二个问题是全连接层的参数个数太多，表现在当前的网络难以部署在手机或者其它的一些嵌入式系统上，例如，VGG16有超过500M的参数，其中80%都是全连接层的参数。

针对全连接层的过拟合的问题，在实际的训练中，dropout层会大大降低过拟合的概率，提高训练的精度；针对参数过多的问题，参数剪枝和参数重用是当前解决这个问题的方案。

图中的每一层都是全连接层，为了减少全连接层的参数个数，我们使用如下的方案：
1，上一层的输出reshape为一维向量；
2，以固定的顺序重新排列这个向量；
3，对这个向量做一维卷积。

    从线性代数的角度讲，全连接层将一个向量空间映射到另一个向量空间，如果映射矩阵W的秩大于输入向量维数，信息不会有丢失。卷积操作从线性代数的角度讲，就是完成上面这个映射操作，而且就卷积神经网络使用的一般方式而言，卷积操作生成的映射矩阵W的秩一般为min{m,n}，m为输入向量维度，n为输出向量维度；因此从向量空间角度讲，这意味着卷积操作的信息丢失仅和输出向量维度相关。对于二者的参数个数而言，全连接层需要的参数个数为m*n，卷积层需要的参数个数为n_oc*n_conv，这里n_oc为输出通道个数，n_conv为卷积核大小，事实上，这些参数还要满足如下的等式，stride为卷积每次移动的距离。
 
也就是说，卷积需要的参数个数是 m*n_oc - n*stride，少于全连接参数个数的n_oc/n。这里的n_oc一般只有n的几分之一到几十分之一。Han等使用参数剪枝方法降低全连接层参数数目，在他们的实验中，全连接层参数可以被压缩到20%,这意味着，在全连接参数矩阵中，仅仅有20%非零元素；这个实验也证明了使用一维卷积的合理性。

    因为压缩感知良好的数据恢复能力，压缩感知是当前非常热门的领域，从编码看，它是一种新的有损压缩编码方式；压缩感知的想法是使用通用的编码方式，特殊的解码方式完成编解码过程。在理论上，压缩一个服从分布p(x)的向量x，压缩感知编码的一种可能做法是对这个向量随机采样，解码时使用L1范数作为代价方程，恢复原向量，可以证明，如果分布符合某种条件，恢复的向量和原始向量的误差可以很小。可以将我们的变换过程看做是一个压缩感知过程。（从实验中看，STL分类中，如果用L1范数代替L2范数，分类的效果可以提高3%）。

    使用重排的另一个原因是，因为卷积核的局部性质，仅仅经过卷积产生的向量只有局部信息，没有全局信息，重排后，对新的向量卷积，根据上面的压缩感知性质，卷积后的向量包含原始向量所有的信息，这样的卷积结果会有更好的全局特性。

    在当前的网络中，常用的分类器是softmax，在分类器之前也有一个全连接层；这个全连接层和其它全连接层的不同之处在于，它的输出维度是确定的，必须和类别数相同（其它的全连接层的输出维度一般由经验自由给定，没有必须的要求）。直接使用上述的过程，会有卷积后输出维度和类别数不匹配的问题，因此，我们在这里接一个全连接层。

     最终的结果是，整个网络只有softmax之前一个全连接层，这会大大减少网络的参数。通过在Mnist上的实验说明了这一点：
