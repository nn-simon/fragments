#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
A self-organizing map is a type of artifical neural network that is trained
 using unsupervised learning to produce a low-dimensional, discretized represent
ation of the input space of the training samples.
\end_layout

\begin_layout Standard
SOMs operate in tow modes: training and mapping and they apply competitive
 learning as opposed to error-correction learning (such as backpropagation
 with gradient descent).
\end_layout

\begin_layout Standard
The goal of learning in the SOM is to cause different parts of the network
 to respond similarly to certain input patterns.
\end_layout

\begin_layout Standard
A SOM consists of components called nodes or neurons.
 Associated with each node are a weight vector of the same dimension as
 the input data vectors, and a position in the map space.
\end_layout

\begin_layout Standard
Best Matching Unit(BMU): the neuron whose weight vector is most similar
 to the input.
 
\end_layout

\begin_layout Standard
The update formula for a neuron 
\begin_inset Formula $v$
\end_inset

 with weight vector 
\begin_inset Formula $W_{v}$
\end_inset

 is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
W_{v}(s+1)=W_{v}(s)+\Theta(u,v,s)\alpha(s)f\big(D(t),W(s)\big)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $s$
\end_inset

 is the step index, 
\begin_inset Formula $t$
\end_inset

 is the index into the training sample, 
\begin_inset Formula $u$
\end_inset

 is the index of the BMU for 
\begin_inset Formula $D(t)$
\end_inset

, 
\begin_inset Formula $\text{\alpha(s)}$
\end_inset

 is a monotonically decreasing learning coefficient, 
\begin_inset Formula $\Theta(u,v,s)$
\end_inset

 is the neighborhood function.
\end_layout

\end_body
\end_document
